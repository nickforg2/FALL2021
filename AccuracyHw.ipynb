{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AccuracyHw.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP4MRP3/6myHDguJul0Erb2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickforg2/FALL2021/blob/main/AccuracyHw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrKwU1wB0eyO"
      },
      "source": [
        "Explain in words what accuracy, precision, and recall are.  Describe a situation when you would prefer one to another and where the shortcomings to each lays. \n",
        "\n",
        "Accuracy is the ability to measure an accurate value. It is essentially how close an insturment can get to a measured value. Precision is the how close two measurements are to eachother and Recall is the calculation of the correctly predicted values. Recall is useful in fraud-detection but probably not accuracy. \n",
        "\n",
        "\n",
        "What is a confusion matrix?\n",
        "A confusion matrix sets up the actual values versus the prediction values. It allows you to see the errors being made and is essentially a table of the corret and incorrect predictions. The main use to measure the performance of a prediction model. \n",
        "\n",
        "Write the python code for accuracy, precision, recall, and F1\n",
        "\n",
        "Accuracy- accuracy = metrics.accuracy_score(y_test, preds)\n",
        "\n",
        "Precision- precision_positive = metrics.precision_score(y_test, preds, pos_label=1) precision_negative = metrics.precision_score(y_test, preds, pos_label=0)\n",
        "\n",
        "F1-y_predict = clf.predict_proba(X_test)[:,1]\n",
        "print f1_score(y_test,y_predict)\n",
        "\n",
        "\n",
        "Give your own example of a type 1 and type 2 error\n",
        "\n",
        "A type 1 error is convicting an innocent defendent. In programming terms, a type 1 error could be sending a shipping notice prior to an order being shipped. \n",
        "\n",
        "A type 2 error would be like getting a negative result on a test for Covid when you actually have Covid. Im not quite sure how this would equate to programming but I'm sure it does! One thing I found was errors and leftovers in statistics. It then says for example, linear regression. \n",
        "\n",
        "Why do we use train.test,split() function from Python when analyzing data?  What is the point of splitting data? The train test split is used to estimate the overall performance of algorithms when they are used for predicitions on data. The point of this is to analyze and test one portion of the system to get an understanding of how the rest of the system may work. The reason for seperating or splitting the data is to minimize the possibility of bias in the evaluation. \n",
        "\n",
        "What is the bias vs. variance tradeoff?\n",
        "\n",
        "Bias is the simplifying assumptions made by the model to make the function easier to understand. Whereas, Variance is the amount that the function will change given different training methods. The trade-off is the tension beteen the error introduced by the bias and the variance. \n",
        "\n"
      ]
    }
  ]
}